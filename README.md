# Distributed AI Runtime

## Project Highlights

âœ… Complete C++17 distributed tensor operations system  
âœ… ThreadPool-based concurrent execution  
âœ… Fault-tolerant networking with dead socket removal  
âœ… Disk-backed checkpointing (KVStore)  
âœ… Neural execution graph with parallel node execution  
âœ… TCP-based tensor broadcasting  
âœ… Task scheduling system

## Architecture

```mermaid
graph TB
    subgraph NodeA["ğŸ”· Node A (Port 5001)"]
        TP_A["ğŸŸ¢ ThreadPool<br/>(Worker Threads)"]
        KV_A["ğŸŸ  KVStore<br/>(In-Memory + Disk)"]
        GN_A["âš™ï¸ Graph Nodes<br/>(Compute Tasks)"]
    end
    
    subgraph NodeB["ğŸ”· Node B (Port 5002)"]
        TP_B["ğŸŸ¢ ThreadPool<br/>(Worker Threads)"]
        KV_B["ğŸŸ  KVStore<br/>(In-Memory + Disk)"]
        GN_B["âš™ï¸ Graph Nodes<br/>(Compute Tasks)"]
    end
    
    subgraph NodeC["ğŸ”· Node C (Port 5003)"]
        TP_C["ğŸŸ¢ ThreadPool<br/>(Worker Threads)"]
        KV_C["ğŸŸ  KVStore<br/>(In-Memory + Disk)"]
        GN_C["âš™ï¸ Graph Nodes<br/>(Compute Tasks)"]
    end
    
    Disk_A[("ğŸ’¾ checkpoints/<br/>latest_tensor.chk")]
    Disk_B[("ğŸ’¾ checkpoints/<br/>latest_tensor.chk")]
    Disk_C[("ğŸ’¾ checkpoints/<br/>latest_tensor.chk")]
    
    NodeA -.->|"ğŸ”´ TCP Broadcast<br/>Tensor Data"| NodeB
    NodeB -.->|"ğŸ”´ TCP Broadcast<br/>Tensor Data"| NodeC
    NodeC -.->|"ğŸ”´ TCP Broadcast<br/>Tensor Data"| NodeA
    
    TP_A -->|Executes| GN_A
    TP_B -->|Executes| GN_B
    TP_C -->|Executes| GN_C
    
    KV_A -->|"saveToDisk()"| Disk_A
    KV_B -->|"saveToDisk()"| Disk_B
    KV_C -->|"saveToDisk()"| Disk_C
    
    Disk_A -.->|"loadFromDisk()<br/>(on startup)"| KV_A
    Disk_B -.->|"loadFromDisk()<br/>(on startup)"| KV_B
    Disk_C -.->|"loadFromDisk()<br/>(on startup)"| KV_C
    
    style NodeA fill:#4A90E2,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style NodeB fill:#4A90E2,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style NodeC fill:#4A90E2,stroke:#2E5C8A,stroke-width:3px,color:#fff
    
    style TP_A fill:#50C878,stroke:#2D7A4A,stroke-width:2px
    style TP_B fill:#50C878,stroke:#2D7A4A,stroke-width:2px
    style TP_C fill:#50C878,stroke:#2D7A4A,stroke-width:2px
    
    style KV_A fill:#FF8C42,stroke:#CC6A2F,stroke-width:2px
    style KV_B fill:#FF8C42,stroke:#CC6A2F,stroke-width:2px
    style KV_C fill:#FF8C42,stroke:#CC6A2F,stroke-width:2px
```

**Components:**

- **ğŸ”· Node (Blue)**: Distributed compute node with TCP server
- **ğŸŸ¢ ThreadPool (Green)**: Concurrent task execution with worker threads
- **ğŸŸ  KVStore (Orange)**: In-memory tensor storage with disk persistence
- **âš™ï¸ Graph Nodes**: Execution graph for neural computation
- **ğŸ”´ Broadcast (Red arrows)**: TCP-based tensor distribution between nodes
- **ğŸ’¾ Checkpoints**: Disk-backed tensor serialization

## Overview

A C++ distributed AI runtime prototype supporting:

- Multi-node tensor broadcast
- Thread-pool task scheduling
- Disk-backed checkpointing
- Minimal neural execution graph

This project demonstrates distributed systems engineering, concurrency, and ML infrastructure in a single, internship-ready repository.

## Screenshots

### Tensor Broadcast & Network Communication

![Broadcast Output](screenshots/broadcast_output.png%20.png)
*Multi-threaded tensor broadcasting between nodes over TCP with fault-tolerant socket management*

### Graph Execution

![Graph Execution](screenshots/graph_execution.png%20.png)
*Parallel execution of computation graph nodes via ThreadPool with thread IDs displayed*

### Checkpoint System

![Checkpoint File](screenshots/checkpoint_file.png%20.png)
*Disk-backed tensor checkpointing with binary serialization (TENS format)*

